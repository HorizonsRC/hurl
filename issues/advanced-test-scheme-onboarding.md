# Advanced Test Scheme & Developer Onboarding for HTTPX Performance/Integration Testing

## Overview
This issue proposes a revised and advanced testing/data scheme to support the complex needs of `hurl`'s performance and integration testing, as well as robust CI/CD. It documents all data sources, test schemes, and recommended onboarding steps for developers working on or with test infrastructure.

## Test Data Sources
1. **Remote Data**: Live data from production/test servers. Contains sensitive information; never checked in. Used to update fixture cache or validate integration.
2. **Cached Remote Data**: Local cache of remote data. Populated by devs for real-world test coverage and speed. Not checked in; not available on CI.
3. **Mocked (Anonymized) Data**: Programmatically generated or anonymized data, checked into the repo. Always available, suitable for CI and onboarding.

## Testing Schemes
- **Fully Mocked Tests**: Use only anonymized, checked-in data. Run in CI. Should provide near-full coverage at the unit level.
- **Cached Tests**: Use local cache of real remote data. Enable fast offline dev iteration with realistic responses. Not available on CI; must be regenerated by devs as needed.
- **Remote Tests**: Directly test against live server(s). Used for cache population and (optionally) for high-fidelity performance/integration testing. Never run in CI.

## Test Suites
- **Unit Tests**: Use only mocked data. Run in CI. Coverage should be maximized here.
- **Integration Tests**: Can be run using mocked or cached data for coverage; can use remote for fixture refresh/validation. No hard-coded server responses.
- **Performance Tests**: Prefer remote data but can use cached/mocked data for offline/CI benchmarking. Use for parsing-vs-latency analysis and detecting regressions.

## Implementation Tasks (in addition to #12)
- [ ] Refactor test/configuration logic to clearly separate and document all three data sources.
- [ ] Ensure all tests and fixtures can be run in the appropriate modes (CI/dev/local/remote).
- [ ] Add logic to skip tests gracefully if required data sources are missing or unavailable (with clear reasons).
- [ ] Add developer onboarding documentation:
    - How to populate the cache, generate anonymized fixtures, and select test modes
    - How to update fixtures and when to use each test scheme
    - Security & privacy guidelines for test data
- [ ] Diagram/test matrix for onboarding docs (see conversation above for example)
- [ ] Document CLI flags and `.env` usage for test selection and server config
- [ ] Link this meta-issue to #12 and PR #13; use this as the coordination point for onboarding, infra, and design discussions

## Onboarding/Dev Documentation Checklist
- [ ] How to run each test scheme (unit, integration, performance) with each test data source (mocked/cached/remote) where appropriate.
- [ ] How to regenerate fixtures
- [ ] Security notes for handling sensitive data
- [ ] How CI/CD runs differ from local/dev
- [ ] How to contribute new scenarios/fixtures

---

This issue is a companion/meta-issue for #12 (integration test features) and PR #13 (feature branch). All onboarding, infra, and scheme design discussion should happen here. When resolved, this will close #12 and mark onboarding and test infra design as complete.